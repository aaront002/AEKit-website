---
layout: page
title: Motivation
---

**Questions**

What question(s) are you answering?
What need are you filling?


In an era of increasing technological advancement, our society now has more access than ever to information and data analysis tools. Algorithmic tools and systems have been introduced to help us process and interpret the multitude of data that our society now has access to. Ultimately, algorithms help people make decisions; however, the increasing use of algorithms may replicate, exacerbate, mask, or transfer inequities in our current system. Extensive evidence demonstrates that the harms of algorithmic and information technologies are significant. Demonstrated harms exist across highly varied applications. Several systems such as automated bail decisions, facial recognition, algorithmically supported hiring decisions have been found to have racial and gender bias. Additionally, automated license plate reader
s lead to unwarranted police stops. Credit card records have been stolen in major privacy breaches. Digital currencies are susceptible to price manipulation. Social media is susceptible to disinformation campaigns.
The use of algorithmic systems and tools also compromises oversight by reducing the ability for the public to access data used in these systems and reducing the ability to interpret the results (Friedman 342) . Our society increasingly depends on the use of algorithms without fully comprehending the assumptions and the issues behind the data used in algorithmic tools or systems. The City of Seattle and Washington State are both world leaders in technology policy, but they do not have policies yet that regulate the applications and processes of algorithmic tools and systems to prevent inequities. The Washington State House has drafted a tech fairness bill (HB 1655) is a first step in the direction of broad algorithmic regulation, but it did not pass (WA Leg HB 1655). Governments agencies like the City of Seattle currently use algorithmic systems and tools, but are not able to recognize them as such or fully comprehend the potential harms of using such technologies. Recent ethnographic research with the City of Seattle affirms that even expert technologists within the city did not consider Automated License Plate Readers or any of the devices on the City’s “Master List” of surveillance technologies as algorithmic systems but focused instead on their data collection functions (Young et al 24). Policy-makers and on-the-ground stakeholders alike find algorithmic systems to be illegible. Risks that are already subject to existing legislation are not being recognized because these risks are algorithmic in nature. Regulation specially tailored to algorithmic systems is needed in order to render automated decision systems accountable to the public. 
The Algorithmic Equity Toolkit project would generate a toolkit that can be adopted both within government and by policy advocates such as the ACLU to strengthen HB 1655 and other existing, ongoing, and future regulatory efforts (WA Leg HB 1655). Beyond the public sector, many grassroots and advocacy organizations desire visibility into systems that could have a disparate impact, but lack domain knowledge and a set of recommended processes for exposing such systems to oversight. Furthermore, such systems are typically “black boxes,” provided by manufacturers who may not be willing to reveal key aspects of their functionality. Even when a system’s functions are well-documented, their potential for disparate impact is not readily apparent (Friedman 5). To remedy this gap in understanding, and to provide those affected with tools necessary to hold algorithmic systems accountable, the Algorithmic Equity Toolkit will be designed to equip non-experts with a process and tools for surfacing unintended impacts of systems in use.


**Stakeholders**

Who are the important stakeholders and what has your team done to take them into consideration?
What are the use cases you’re building for?

**Ethics**

What are the ethical questions you considered as a team?
How are you addressing them in your work?
